{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c11d8c65",
   "metadata": {},
   "source": [
    "# Explicação rápida dos dados (resumo):\n",
    "\n",
    "- Registros: clientes (customer_id) com variáveis demográficas, comportamentais, transacionais, datas, texto e target churn.\n",
    "- Numéricas: age, income (forte skew + outliers), tenure_months, num_transactions, avg_transaction. \n",
    "    - skew se refere a desequilibrios ou distorções na distribuição.\n",
    "    - outliers são valores extremos que podem distorcer análises estatísticas. Em termos gerais, são dados que fogem do padrão esperado.\n",
    "- Categóricas: country, product, device (BR tem peso maior).\n",
    "- Datas: signup_date, last_login — permitem criar recência, idade da conta, frequência.\n",
    "- Texto: review_text (comentários curtos, útil para TF-IDF/embeddings). -> análise de sentimentos, tópicos.\n",
    "- Target: churn (binário, desequilibrado — ~10% base). -> prever abandono.\n",
    "- Problemas artificiais incluídos: missing (income, last_login, review_text), outliers extremos em income/avg_transaction, duplicatas, distribuição enviesada.\n",
    "    - Esses problemas simulam desafios reais em dados de clientes.\n",
    "\n",
    "# Tarefa proposta (prática de feature engineering + baseline):\n",
    "\n",
    "### Objetivo: montar um pipeline reprodutível que faça engenharia de features, treine um modelo simples e reporte métricas.\n",
    "\n",
    "> Passos mínimos (sugestão):\n",
    "\n",
    "1. Exploração\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "> summary estatistico: refere-se a uma análise descritiva dos dados que inclui medidas como média, mediana, desvio padrão, valores mínimos e máximos, entre outros.\n",
    "- usa-se: \n",
    "    - ```df.describe()``` no pandas.\n",
    "\n",
    "> distribuições: refere-se à forma como os dados estão distribuídos para cada variável. \n",
    "- Isso pode incluir:\n",
    "    - histogramas:\n",
    "        - usa-se:\n",
    "            - ````sns.histplot()```` do seaborn, ````plt.hist()```` do matplotlib.\n",
    "    - boxplots:\n",
    "        - usa-se:\n",
    "            - ````sns.boxplot()```` do seaborn, ````plt.boxplot()```` do matplotlib.\n",
    "    - KDE plots (Kernel Density Estimation):\n",
    "        - usa-se:\n",
    "            - ````sns.kdeplot()```` do seaborn.\n",
    "\n",
    "> correlações: medem quanto uma variável se relaciona com outra.\n",
    "    > A matriz de correlação ````df.corr()```` mostra valores entre -1 e 1, indicando a força e direção da relação.\n",
    "- +1: correlação positiva perfeita (ambas crescem juntas)\n",
    "- 1: correlação negativa perfeita (uma cresce, outra diminui)\n",
    "- 0: sem relação linear clara\n",
    "- usa-se:\n",
    "    - ````sns.heatmap()```` do seaborn para visualizar a matriz de correlação.\n",
    "\n",
    "> contagem por categoria: refere-se à contagem de ocorrências para cada categoria em variáveis categóricas.\n",
    "- usa-se:\n",
    "    - ````df['categoria'].value_counts()```` no pandas.\n",
    "\n",
    "> percentuais de nulos: refere-se à proporção de valores ausentes em cada coluna do dataset.\n",
    "- usa-se:\n",
    "    - ````df.isnull().mean() * 100```` no pandas. -> lê-se como \"percentual de valores nulos por coluna\".\n",
    "\n",
    "\n",
    "> checar duplicatas e formato de datas: refere-se à verificação de registros duplicados no dataset e à validação do formato das colunas de data.\n",
    "- importa pois:\n",
    "    - duplicatas podem ganhar peso indevido.\n",
    "    - datas mal formatadas impedem cálculos corretos de recência, idade da conta, etc. \n",
    "        - exemplo: \n",
    "            - “2025/01/02”, “02-01-2025” e “01-02-25” podem representar coisas diferentes dependendo do padrão adotado (americano vs brasileiro).\n",
    "            - converter signup_date/last_login para datetime.\n",
    "- usa-se para duplicatas:\n",
    "\n",
    "    ```python\n",
    "     # para contar duplicatas.\n",
    "    df.duplicated().sum()\n",
    "\n",
    "     # para visualizar duplicatas.\n",
    "    df[df.duplicated()]\n",
    "\n",
    "    # para remover duplicatas.\n",
    "    df.drop_duplicates()\n",
    "\n",
    "    # para remover duplicatas parcialmente\n",
    "    df = df.drop_duplicates(subset=['customer_id', 'signup_date'])\n",
    "    ```\n",
    "\n",
    "- usa-se para datas:\n",
    "    ```python\n",
    "    # Converter coluna de string para datetime\n",
    "    df['data'] = pd.to_datetime(df['data'], errors='coerce', dayfirst=True)\n",
    "    \n",
    "    # Checar valores inválidos (que viraram NaT)\n",
    "    df['data'].isna().sum()\n",
    "    \n",
    "    # Extrair partes úteis da data\n",
    "    df['ano'] = df['data'].dt.year\n",
    "    df['mes'] = df['data'].dt.month\n",
    "    df['dia_semana'] = df['data'].dt.day_name()\n",
    "\n",
    "    # signup_date/last_login\n",
    "    df['signup_date'] = pd.to_datetime(df['signup_date'], errors='coerce', dayfirst=True)\n",
    "    df['last_login'] = pd.to_datetime(df['last_login'], errors='coerce', dayfirst=True)\n",
    "\n",
    "    ```\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "2. Limpeza\n",
    "    - remover duplicatas (total ou parcial por customer_id + signup_date).\n",
    "    - converter signup_date/last_login para datetime.\n",
    "    - criar coluna boolean indicando se last_login é nulo.\n",
    "    - exemplo: \n",
    "    ```python\n",
    "    df['no_login'] = df['last_login'].isna() # isna() retorna True para valores nulos\n",
    "    ```\n",
    "    <br><br>\n",
    "3. Features temporais\n",
    "    > recency = (today - last_login).days (usar um referência fixa para reprodutibilidade).\n",
    "    - Conceito: Mede a quantidade de dias que se passaram desde a última interação (neste caso, o último login) do usuário até a data de hoje.\n",
    "    - Interpreção: Um valor baixo indica que o usuário fez login recentemente e está engajado.Um valor alto indica que o usuário está inativo ou pode estar em risco de churn (abandono).\n",
    "    uso: \n",
    "    ```python\n",
    "    df['recency'] = (pd.Timestamp(\"2025-10-01\") - df['last_login']).dt.days # exemplo com data fixa\n",
    "    ```\n",
    "    <br><br>\n",
    "    > account_age_days = (today - signup_date).days.\n",
    "    - Conceito: Mede a idade total da conta do usuário em dias, desde a data de cadastro (signup_date) até hoje.\n",
    "    - Interpretação: Um valor alto indica que o usuário é um cliente antigo, o que pode sugerir lealdade. Um valor baixo indica um cliente novo, que pode estar em fase de avaliação do serviço.\n",
    "    uso: \n",
    "    ```python\n",
    "    df['account_age_days'] = (pd.Timestamp(\"2025-10-01\") - df['signup_date']).dt.days # exemplo com data fixa\n",
    "    ```\n",
    "    <br><br>\n",
    "    > activity_rate = num_transactions / max(1, account_age_days/30).\n",
    "    - Conceito: Mede a frequência média de transações do usuário por mês, normalizando pelo tempo que a conta está ativa.\n",
    "    - Detalhes da fórmula:\n",
    "        - num_transactions: O número total de transações feitas pelo usuário.\n",
    "        - account_age_days/30: Converte a idade da conta em dias para a idade da conta em meses.\n",
    "        max(1, ...): É uma técnica de proteção contra divisão por zero ou por um período de tempo muito pequeno. Garante que o denominador seja pelo menos 1, o que é crucial para contas criadas há menos de um mês (para evitar uma taxa de atividade inflacionada ou divisão por zero).\n",
    "    - Interpretação: Um valor alto indica um usuário ativo que realiza transações regularmente. Um valor baixo pode indicar um usuário inativo ou com baixo engajamento.\n",
    "    uso: \n",
    "    ```python\n",
    "    # Calcula a idade em meses, garantindo que o divisor seja no mínimo 1\n",
    "    age_months = df_usuarios['account_age_days'] / 30\n",
    "    divisor = np.maximum(1, age_months)\n",
    "\n",
    "    df_usuarios['activity_rate'] = df_usuarios['num_transactions'] / divisor\n",
    "    ```\n",
    "\n",
    "4. Tratamento de missing\n",
    "    > income: Preencher (os valores ausentes) com a mediana e criar uma flag chamada is_income_missing.\n",
    "    > last_login: Preencher os valores ausentes usando signup_date (ou um valor que represente grande recency) e criar uma flag indicando que o valor foi imputado.\n",
    "    > review_text: Substituir valores ausentes ou vazios por uma string vazia (\"\") e criar uma coluna indicando que o valor original estava faltando.\n",
    "\n",
    "5. Outliers\n",
    "    > Outliers são valores muito altos ou muito baixos que fogem ao padrão da maioria dos dados.\n",
    "    > truncamento (winsorize) ou log-transform para income e avg_transaction; criar versão transformada.\n",
    "        - truncamento (winsorize): É uma técnica que limita os valores extremos a um certo percentil. Em vez de remover outliers, você os “trunca” nos limites superior e inferior.\n",
    "            - uso:\n",
    "            ```python\n",
    "            from scipy.stats.mstats import winsorize\n",
    "            df['income_winsorized'] = winsorize(df['income'], limits=[0.01, 0.01])\n",
    "            ```\n",
    "        - log-transform: Aplica o logaritmo nos valores — útil para variáveis altamente assimétricas (skewed), como renda ou gasto médio.\n",
    "            - uso: \n",
    "            ```python\n",
    "            import numpy as np\n",
    "            df['income_log'] = np.log1p(df['income'])\n",
    "            df['avg_transaction_log'] = np.log1p(df['avg_transaction'])\n",
    "            ```\n",
    "6. Encoding\n",
    "    > Endcoding é o processo de transformar variáveis categóricas ou texto em formatos numéricos que modelos de machine learning podem entender.\n",
    "    - categóricas: One-Hot para device/product; target/mean-encoding ou frequency-encoding para country (avaliar leakage).\n",
    "        - One-Hot Encoding: Cria uma nova coluna binária para cada categoria.\n",
    "            - uso:\n",
    "            ```python\n",
    "            import pandas as pd\n",
    "            pd.get_dummies(df, columns=['device', 'product'])\n",
    "            ```\n",
    "        - Target/Mean-Encoding: Substitui cada categoria pelo valor médio da variável-alvo (target) para essa categoria.\n",
    "            - uso:\n",
    "                ```python\n",
    "                mean_target = df.groupby('country')['churn'].mean()\n",
    "                df['country_mean_encoded'] = df['country'].map(mean_target)\n",
    "                ```\n",
    "        - Frequency-Encoding: Substitui cada categoria pela frequência (contagem) dessa categoria no dataset.\n",
    "            - uso:\n",
    "            ```python\n",
    "            freq = df['country'].value_counts()\n",
    "            df['country_freq_encoded'] = df['country'].map(freq)\n",
    "            ```\n",
    "    - texto: TF-IDF (unigram/bigram) limitando vocab (top k) ou usar hash vectorizer.\n",
    "        - TF-IDF (Term Frequency - Inverse Document Frequency): Transforma o texto em vetores numéricos, com base na importância das palavras.\n",
    "            - Palavras muito frequentes no texto → peso maior\n",
    "            - Mas se aparecem em todos os textos → peso menor\n",
    "                - uso:\n",
    "                ```python\n",
    "                from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "                vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "                # ngram_range=(1,2) → considera unigramas (1 palavra) e bigramas (pares de palavras).\n",
    "                # max_features=500 → limita o vocabulário às 500 palavras mais relevantes.\n",
    "                X_tfidf = vectorizer.fit_transform(df['review_text'])\n",
    "                ```\n",
    "        - Hash Vectorizer: Similar ao TF-IDF, mas usa uma função de hash para mapear palavras em um espaço de características fixo, economizando memória.\n",
    "            - uso:\n",
    "            ```python\n",
    "            from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "            vectorizer = HashingVectorizer(n_features=5000, ngram_range=(1, 2))\n",
    "            X_hashed = vectorizer.fit_transform(df['review_text'])\n",
    "            ```\n",
    "7. Pipeline e seleção\n",
    "    - Pipeline: refere-se a uma sequência de etapas de processamento de dados e modelagem que são encadeadas juntas para formar um fluxo de trabalho completo.\n",
    "    - Seleção: refere-se ao processo de escolher um subconjunto de features (variáveis) que são mais relevantes para o modelo, com o objetivo de melhorar o desempenho e reduzir a complexidade.\n",
    "    - montar sklearn ColumnTransformer + Pipeline (imputer, scaler, encoders, text vect).\n",
    "        - Esses dois componentes são a espinha dorsal de um pipeline de ML bem estruturado.\n",
    "        - ColumnTransformer: Permite aplicar transformações diferentes para colunas diferentes no mesmo dataset.\n",
    "        - uso: [ColumnTransformer Example](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "        - Pipeline: Encadeia várias etapas de processamento e modelagem em uma única entidade.\n",
    "        - uso: [Pipeline Example](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "    - reduzir dimensionalidade texto com TruncatedSVD se usar TF-IDF.\n",
    "        - TruncatedSVD (Singular Value Decomposition truncada): É uma técnica de redução de dimensionalidade que é especialmente útil para dados esparsos, como os gerados por TF-IDF.\n",
    "        - uso:\n",
    "        ```python\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "        svd = TruncatedSVD(n_components=100)  # Reduz para 100 dimensões\n",
    "        X_reduced = svd.fit_transform(X_tfidf)\n",
    "        ```\n",
    "8. Entrega dos features\n",
    "    - salvar features processadas: df_features.to_csv('datasets/processed_features.csv') ou pickle\n",
    "    - documentar no notebook as decisões de imputação e outliers\n",
    "<br>\n",
    "### Dicas rápidas de implementação:\n",
    "- use sklearn.compose.ColumnTransformer, sklearn.pipeline.Pipeline.\n",
    "- para texto: sklearn.feature_extraction.text.TfidfVectorizer + TruncatedSVD.\n",
    "- para avaliação temporal: sort by signup_date e use TimeSeriesSplit ou cutoff manual.\n",
    "- fixe \"today\" com uma data constante (ex.: pd.Timestamp(\"2025-10-01\")) para reprodutibilidade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
